{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-07-21'\n",
    "end_date = '2024-10-30'\n",
    "\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the bounding box of the study area\n",
    "val_polys = gpd.read_file(Path('../data/validation-locations/2023-24-insitu-poly.geojson'))\n",
    "val_polys = val_polys.drop_duplicates(subset='GRAND_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = Path(\"../data/icesat2/raw/271707654/processed_ATL13_20221110080859_07731701_006_01.nc\")\n",
    "ds = xr.open_dataset(fp, group='gt1r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "def process_icesat2(fn, save_dir=Path(\"../data/icesat2/processed\")):\n",
    "    dfs = []\n",
    "    fn = Path(fn)\n",
    "\n",
    "    if fn.suffix == '.h5':\n",
    "        f = h5py.File(fn, mode='r')\n",
    "        for gp in ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']:\n",
    "            # check if data exists\n",
    "            if 'segment_lat' not in f[gp].keys():\n",
    "                print(f\"Data in {gp} doesn't exist - {fn}\")\n",
    "            else:\n",
    "                segment_lat = f[gp]['segment_lat'][:]\n",
    "                segment_lon = f[gp]['segment_lon'][:]\n",
    "                ht_ortho = f[gp]['ht_ortho'][:]\n",
    "                ht_water_surf = f[gp]['ht_water_surf'][:]\n",
    "                delta_time = f[gp]['delta_time'][:]\n",
    "\n",
    "                df = pd.DataFrame({\n",
    "                    'segment_lat': segment_lat, 'segment_lon': segment_lon, \n",
    "                    'ht_ortho': ht_ortho, 'ht_water_surf': ht_water_surf,\n",
    "                    'delta_time': delta_time\n",
    "                })\n",
    "                dfs.append(df)\n",
    "    else:\n",
    "        for gp in ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']:\n",
    "            try:\n",
    "                gp = xr.open_dataset(fn, group=gp) # other groups: gt1l, gt1r, gt2l, gt2r, gt3l, gt3r\n",
    "                if len(gp) == 0:\n",
    "                    continue\n",
    "                gp_df = gp[['segment_lat', 'segment_lon', 'delta_time', 'ht_ortho', 'ht_water_surf']].to_dataframe().reset_index()\n",
    "                dfs.append(gp_df)\n",
    "            except:\n",
    "                print(f\"ERROR: {gp}, {fn}\")\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Create a GeoDataFrame from the processed data\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.segment_lon, df.segment_lat), crs=\"EPSG:4326\")\n",
    "    \n",
    "    val_polys_buffered = val_polys.copy()\n",
    "    val_polys_buffered['geometry'] = val_polys_buffered['geometry'].buffer(0.005)\n",
    "\n",
    "    # Perform spatial join to find which polygon each point falls inside\n",
    "    joined = gpd.sjoin(gdf, val_polys_buffered, how=\"left\", predicate='within')\n",
    "\n",
    "    # Add the tmsos_id to the dataframe\n",
    "    df['tmsos_id'] = joined['tmsos_id']\n",
    "\n",
    "    df.rename({\n",
    "        'delta_time': 'time'\n",
    "    }, axis=1, inplace=True)\n",
    "\n",
    "    tmsos_ids = joined['tmsos_id'].unique()\n",
    "    for tmsos_id in tmsos_ids:\n",
    "        if type(tmsos_id) == float:  # tmsos_id is a nan\n",
    "            print(tmsos_id)\n",
    "            continue\n",
    "        save_fp = save_dir / tmsos_id / f\"{fn.stem}.csv\"\n",
    "        save_fp.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        df[df['tmsos_id'] == tmsos_id].to_csv(save_fp, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "gdf = process_icesat2(\"../data/icesat2/raw/277739526/processed_ATL13_20220818202945_08851601_006_01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS = []\n",
    "\n",
    "for d in tqdm(list(Path(\"../data/icesat2/raw\").glob('*'))):\n",
    "    fns = list(d.glob(\"*\"))\n",
    "\n",
    "    for fn in fns:\n",
    "        processed_tracker = open(\"../data/icesat2/processed.txt\", 'r')\n",
    "        processed_list = [l.strip('\\n') for l in processed_tracker.readlines()]\n",
    "        processed_tracker.close()\n",
    "\n",
    "        if str(fn) in processed_list:\n",
    "            print(f\"Skipping {fn}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Processing {fn}\")\n",
    "            df = process_icesat2(fn)\n",
    "\n",
    "            f = open(\"../data/icesat2/processed.txt\", 'a')\n",
    "            f.write(str(fn) + '\\n')\n",
    "\n",
    "            DFS.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVOIR = '1078'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pts = gpd.read_file(Path('../data/validation-locations/2023-24-insitu-pts.geojson'))\n",
    "val_polys = gpd.read_file(Path('../data/validation-locations/2023-24-insitu-poly.geojson'))\n",
    "\n",
    "selected_reservoirs = val_pts['tmsos_id'].tolist()  # select all 100 reservoirs\n",
    "res_names = val_pts[['tmsos_id', 'name']].set_index('tmsos_id').to_dict()['name'] # dictionary that can be queried to get reservoir name\n",
    "\n",
    "RESERVOIR_NAME = res_names[RESERVOIR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aec_dir = Path('../data/aec/aev')\n",
    "\n",
    "aec_fp = aec_dir / f'{RESERVOIR}.csv'\n",
    "aec = pd.read_csv(aec_fp, parse_dates=True, comment='#')\n",
    "aec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egm_96_to_08_conversion(raster_path, tmsosid, val_pts):\n",
    "    \"\"\"\n",
    "    Samples the value within a raster file at the location corresponding to the given tmsosid.\n",
    "\n",
    "    Args:\n",
    "        raster_path (str): File path of the raster EGM96-EGM08.\n",
    "        tmsosid (str): The tmsosid to get the latitude and longitude values.\n",
    "        val_pts (gpd.GeoDataFrame): GeoDataFrame containing the validation points with 'tmsos_id' and 'geometry' columns.\n",
    "\n",
    "    Returns:\n",
    "        float: The sampled value from the raster at the specified location.\n",
    "    \"\"\"\n",
    "    # Get the geometry for the given tmsosid\n",
    "    location = val_pts[val_pts['tmsos_id'] == tmsosid]\n",
    "    if location.empty:\n",
    "        raise ValueError(f\"No location found for tmsosid: {tmsosid}\")\n",
    "\n",
    "    geometry = location.iloc[0]['geometry']\n",
    "    longitude, latitude = geometry.x, geometry.y\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Convert latitude and longitude to the raster's coordinate system\n",
    "        coords = [(longitude, latitude)]\n",
    "\n",
    "        # Sample the raster value at the transformed coordinates\n",
    "        sampled_value = list(src.sample(coords))[0][0]\n",
    "\n",
    "    return sampled_value\n",
    "\n",
    "\n",
    "def get_area_elevation_storage(aev, tmsosid, raster_path=None, area=None, elevation=None, convert_to_egm08=False):\n",
    "    \"\"\"\n",
    "    Interpolates the AEV (Area-Elevation-Volume) curve to return the elevation, area, and storage for an array of values.\n",
    "    Converts the elevation from EGM96 to EGM08 if required.\n",
    "\n",
    "    Args:\n",
    "        aev (pd.DataFrame): DataFrame containing the AEV curve with columns 'Elevation', 'CumArea', and 'Storage'.\n",
    "        tmsosid (str): The tmsosid to get the latitude and longitude values for EGM96 to EGM08 conversion.\n",
    "        raster_path (str, optional): File path of the raster EGM96-EGM08. Required if convert_to_egm08 is True.\n",
    "        area (array-like, optional): Array of area values for interpolation. Defaults to None.\n",
    "        elevation (array-like, optional): Array of elevation values for interpolation. Defaults to None.\n",
    "        convert_to_egm08 (bool, optional): Whether to convert elevation from EGM96 to EGM08. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing arrays of the interpolated 'elevation', 'area', and 'storage'.\n",
    "    \"\"\"\n",
    "    if area is None and elevation is None:\n",
    "        raise ValueError(\"Either 'area' or 'elevation' must be provided\")\n",
    "\n",
    "    if area is not None and elevation is not None:\n",
    "        raise ValueError(\"Only one of 'area' or 'elevation' should be provided\")\n",
    "\n",
    "    if convert_to_egm08:\n",
    "        if raster_path is None or tmsosid is None:\n",
    "            raise ValueError(\"raster_path and tmsosid must be provided for EGM96 to EGM08 conversion\")\n",
    "        egm96_to_08_offset = egm_96_to_08_conversion(raster_path, tmsosid, val_pts)\n",
    "    else:\n",
    "        egm96_to_08_offset = 0\n",
    "\n",
    "    if elevation is not None:\n",
    "        elevations = np.array(elevation)\n",
    "        if convert_to_egm08:\n",
    "            elevations = elevations - egm96_to_08_offset\n",
    "        areas = np.interp(elevations, aev['Elevation'], aev['CumArea'])\n",
    "        storages = np.interp(elevations, aev['Elevation'], aev['Storage'])\n",
    "    else:\n",
    "        areas = np.array(area)\n",
    "        elevations = np.interp(areas, aev['CumArea'], aev['Elevation'])\n",
    "        if convert_to_egm08:\n",
    "            elevations = elevations - egm96_to_08_offset\n",
    "        storages = np.interp(areas, aev['CumArea'], aev['Storage'])\n",
    "\n",
    "    return {\n",
    "        'elevation': elevations,\n",
    "        'area': areas,\n",
    "        'storage': storages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_elevation_storage(aev, tmsosid=None, raster_path=None, area=None, elevation=None, convert_to_egm08=False):\n",
    "    \"\"\"\n",
    "    Interpolates the AEV (Area-Elevation-Volume) curve to return the elevation, area, and storage for an array of values.\n",
    "    Converts the elevation from EGM96 to EGM08 if required.\n",
    "\n",
    "    Args:\n",
    "        aev (pd.DataFrame): DataFrame containing the AEV curve with columns 'Elevation', 'CumArea', and 'Storage'.\n",
    "        tmsosid (str): The tmsosid to get the latitude and longitude values for EGM96 to EGM08 conversion. Defaults to None.\n",
    "        raster_path (str, optional): File path of the raster EGM96-EGM08. Required if convert_to_egm08 is True.\n",
    "        area (array-like, optional): Array of area values for interpolation. Defaults to None.\n",
    "        elevation (array-like, optional): Array of elevation values for interpolation. Defaults to None.\n",
    "        convert_to_egm08 (bool, optional): Whether to convert elevation from EGM96 to EGM08. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing arrays of the interpolated 'elevation', 'area', and 'storage'.\n",
    "    \"\"\"\n",
    "    if area is None and elevation is None:\n",
    "        raise ValueError(\"Either 'area' or 'elevation' must be provided\")\n",
    "\n",
    "    if area is not None and elevation is not None:\n",
    "        raise ValueError(\"Only one of 'area' or 'elevation' should be provided\")\n",
    "\n",
    "    if convert_to_egm08:\n",
    "        if raster_path is None or tmsosid is None:\n",
    "            raise ValueError(\"raster_path and tmsosid must be provided for EGM96 to EGM08 conversion\")\n",
    "        egm96_to_08_offset = egm_96_to_08_conversion(raster_path, tmsosid, val_pts)\n",
    "    else:\n",
    "        egm96_to_08_offset = 0\n",
    "\n",
    "    if elevation is not None:\n",
    "        elevations = np.array(elevation)\n",
    "        if convert_to_egm08:\n",
    "            elevations = elevations - egm96_to_08_offset\n",
    "        areas = np.interp(elevations, aev['Elevation'], aev['CumArea'])\n",
    "        storages = np.interp(elevations, aev['Elevation'], aev['Storage'])\n",
    "    else:\n",
    "        areas = np.array(area)\n",
    "        elevations = np.interp(areas, aev['CumArea'], aev['Elevation'])\n",
    "        if convert_to_egm08:\n",
    "            elevations = elevations - egm96_to_08_offset\n",
    "        storages = np.interp(areas, aev['CumArea'], aev['Storage'])\n",
    "\n",
    "    return {\n",
    "        'elevation': elevations,\n",
    "        'area': areas,\n",
    "        'storage': storages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = Path(\"../data/icesat2/processed\")\n",
    "directories = list(processed_dir.glob(\"*\"))\n",
    "print(len(directories))\n",
    "\n",
    "for d in directories:\n",
    "    RESERVOIR = d.stem\n",
    "    RESERVOIR_NAME = res_names[RESERVOIR]\n",
    "    print(RESERVOIR, RESERVOIR_NAME)\n",
    "\n",
    "    aec_fp = aec_dir / f'{RESERVOIR}.csv'\n",
    "    if not aec_fp.exists():\n",
    "        continue\n",
    "    aec = pd.read_csv(aec_fp, parse_dates=True, comment='#')\n",
    "    \n",
    "    files = list(d.glob(\"*\"))\n",
    "    def read_elevations(f):\n",
    "        df = pd.read_csv(f, dtype={'tmsos_id': str})\n",
    "        R = None\n",
    "\n",
    "        try:\n",
    "            t = pd.to_datetime(df['time'])\n",
    "            df['time'] = t\n",
    "            R = df\n",
    "        except:\n",
    "            R = None\n",
    "        \n",
    "        return R\n",
    "\n",
    "    df = pd.concat([read_elevations(f) for f in files])\n",
    "    df = df[df['time'] > pd.to_datetime('2023-01-01')]\n",
    "    df['date'] = df['time'].dt.date\n",
    "\n",
    "    avg_elevation = df.groupby('date').agg({\n",
    "        'ht_ortho': 'median'\n",
    "    })\n",
    "\n",
    "    aes = get_area_elevation_storage(aec, elevation=avg_elevation['ht_ortho'])\n",
    "    storage = aes['storage']\n",
    "    area = aes['area']\n",
    "\n",
    "    storage_df = avg_elevation.copy().rename({'ht_ortho': 'elevation'}, axis=1)\n",
    "    storage_df['storage'] = storage\n",
    "    storage_df['area'] = area\n",
    "\n",
    "    save_fp = Path(f\"../data/storage/icesat2/median_elevations/\") / f\"{RESERVOIR}_{RESERVOIR_NAME.split(',')[0].replace(' ', '_')}_storage.csv\"\n",
    "    storage_df.to_csv(save_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
